services:
  generator:
    container_name: generator
    build: generator/

  airflow-webserver:
    container_name: airflow-webserver
    image: apache/airflow:latest
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://$AIRFLOW_POSTGRES_USERNAME:$AIRFLOW_POSTGRES_PASSWORD@airflow-postgres:5432/$AIRFLOW_POSTGRES_DB
      - AIRFLOW__WEBSERVER__SECRET_KEY=$AIRFLOW_WEBSERVER_SECRET_KEY
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0
      - _PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS:- apache-airflow-providers-apache-kafka}
      - AIRFLOW_VAR_KAFKA_TOPIC_PERSON=$KAFKA_TOPIC_PERSON
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    volumes:
      - ./airflow-dags:/opt/airflow/dags
    ports:
      - $AIRFLOW_WEBSERVER_PORT:8080
    command: >
      bash -c "
        airflow db init &&
        if ! airflow users list | grep -q $AIRFLOW_USERNAME; then
          airflow users create --username $AIRFLOW_USERNAME --firstname Admin --lastname User --role Admin --email admin@example.com --password $AIRFLOW_PASSWORD
        fi &&
        airflow webserver
      "
    depends_on:
      - airflow-postgres
      - airflow-scheduler
      - airflow-redis
      - generator
      - kafka-broker

  airflow-scheduler:
    container_name: airflow-scheduler
    image: apache/airflow:latest
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://$AIRFLOW_POSTGRES_USERNAME:$AIRFLOW_POSTGRES_PASSWORD@airflow-postgres:5432/$AIRFLOW_POSTGRES_DB
      - AIRFLOW__WEBSERVER__SECRET_KEY=$AIRFLOW_WEBSERVER_SECRET_KEY
      - _PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS:- apache-airflow-providers-apache-kafka}
      - AIRFLOW_VAR_KAFKA_TOPIC=$KAFKA_TOPIC_PERSON
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    volumes:
      - ./airflow-dags:/opt/airflow/dags
    command: airflow scheduler
    depends_on:
      - airflow-postgres

  airflow-postgres:
    container_name: airflow-postgres
    image: postgres:latest
    environment:
      - POSTGRES_USER=$AIRFLOW_POSTGRES_USERNAME
      - POSTGRES_PASSWORD=$AIRFLOW_POSTGRES_PASSWORD
      - POSTGRES_DB=$AIRFLOW_POSTGRES_DB

  airflow-redis:
    container_name: airflow-redis
    image: redis:latest
  
  kafka-console:
    container_name: kafka-console
    image: redpandadata/console:latest
    ports:
      - $KAFKA_CONSOLE_PORT:8080
    environment:
      - KAFKA_BROKERS=kafka-broker:29092
    depends_on:
      - kafka-broker

  kafka-broker:
    container_name: kafka-broker
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: kafka-zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    depends_on:
      - kafka-zookeeper

  kafka-zookeeper:
    container_name: kafka-zookeeper
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka-init:
    container_name: kafka-init
    image: confluentinc/cp-kafka:latest
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until kafka is reachable
      kafka-topics --bootstrap-server kafka-broker:29092 --list

      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server kafka-broker:29092 --create --if-not-exists --topic $KAFKA_TOPIC_PERSON --replication-factor 1 --partitions 1
    
      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafka-broker:29092 --list
      "
    depends_on:
      - kafka-broker

  ksqldb-server:
    container_name: ksqldb-server
    image: confluentinc/cp-ksqldb-server:latest
    hostname: ksqldb-server
    healthcheck:
      test: curl -f http://ksqldb-server:8088/ || exit 1
    environment:
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka-broker:29092
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"
    depends_on:
      - kafka-broker

  ksqldb-init:
    container_name: ksqldb-init
    image: confluentinc/cp-ksqldb-cli:latest
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Creating ksqlDB streams and tables'
      ls -ltr '/scripts/';
      ksql http://ksqldb-server:8088 <<EOF
      RUN SCRIPT '/scripts/ksqldb-init.sql';
      EOF
      "
    volumes:
      - ./ksql-scripts/:/scripts/
    depends_on:
      ksqldb-server:
        condition: service_healthy

  # docker exec -it ksqldb-cli ksql http://ksqldb-server:8088
  ksqldb-cli:
    container_name: ksqldb-cli
    image: confluentinc/cp-ksqldb-cli:latest
    entrypoint: /bin/sh
    tty: true
    depends_on:
      - kafka-broker
      - ksqldb-server

  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.5.1
    container_name: kafka-connect
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-broker:29092
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_GROUP_ID: "kafka-connect-s3"
      CONNECT_CONFIG_STORAGE_TOPIC: "kafka-connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "kafka-connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "kafka-connect-status"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components
      AWS_ACCESS_KEY_ID: $MINIO_ACCESS_KEY
      AWS_SECRET_ACCESS_KEY: $MINIO_SECRET_KEY
    ports:
      - 8083:8083
    healthcheck:
      test: curl -f http://kafka-connect:8083 | grep -q 'kafka_cluster_id' || exit 1
    command:
      - bash
      - -c
      - |
        echo "Installing S3 Connector"
        confluent-hub install --no-prompt confluentinc/kafka-connect-s3:10.5.19
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity
    depends_on:
      - kafka-broker
      - minio

  kafka-connect-ui:
    image: landoop/kafka-connect-ui:latest
    container_name: kafka-connect-ui
    environment:
      CONNECT_URL: http://kafka-connect:8083
    ports:
      - 8082:8000
    depends_on:
      - kafka-connect
  
  minio:
    image: minio/minio:latest
    container_name: minio
    environment:
      MINIO_ROOT_USER: $MINIO_ACCESS_KEY
      MINIO_ROOT_PASSWORD: minioadmin
    command: server --console-address :9001 /data
    healthcheck:
      test: curl -f http://minio:9000/minio/health/ready || exit 1
    ports:
      - 9000:9000
      - 9001:9001

  minio-init:
    container_name: minio-init
    image: quay.io/minio/mc
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set minio http://minio:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY &&
      /usr/bin/mc mb minio/$MINIO_BUCKET_PERSON &&
      /usr/bin/mc mb minio/$MINIO_BUCKET_STATS || true
      "
    depends_on:
      minio:
        condition: service_healthy
  
  s3-connector-init:
    container_name: s3-connector-init
    image: curlimages/curl:latest
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for Kafka Connect...';
      while ! curl -s http://kafka-connect:8083/ | grep -q 'kafka_cluster_id'; do
        sleep 5;
        echo 'Waiting for Kafka Connect to be available...';
      done;
      echo 'Kafka Connect is ready. Configuring S3 Sink Connector...';
      curl -X POST -H 'Content-Type: application/json' --data '{
        \"name\": \"s3-sink-connector-person\",
        \"config\": {
          \"connector.class\": \"io.confluent.connect.s3.S3SinkConnector\",
          \"tasks.max\": \"1\",
          \"topics\": \"$KAFKA_TOPIC_PERSON\",
          \"s3.bucket.name\": \"$MINIO_BUCKET_PERSON\",
          \"s3.region\": \"us-east-1\",
          \"store.url\": \"http://minio:9000\",
          \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",
          \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",
          \"storage.class\": \"io.confluent.connect.s3.storage.S3Storage\",
          \"format.class\": \"io.confluent.connect.s3.format.json.JsonFormat\",
          \"value.converter.schemas.enable\": \"false\",
          \"flush.size\": \"10\",
          \"rotate.interval.ms\": \"60000\",
          \"aws.access.key.id\": \"$MINIO_ACCESS_KEY\",
          \"aws.secret.key\": \"$MINIO_SECRET_KEY\",
          \"aws.credentials.provider.class\": \"com.amazonaws.auth.BasicAWSCredentialsProvider\"
        }
      }' http://kafka-connect:8083/connectors;
      curl -X POST -H 'Content-Type: application/json' --data '{
        \"name\": \"s3-sink-connector-person-stats\",
        \"config\": {
          \"connector.class\": \"io.confluent.connect.s3.S3SinkConnector\",
          \"tasks.max\": \"1\",
          \"topics\": \"$KAFKA_TOPIC_STATS\",
          \"s3.bucket.name\": \"$MINIO_BUCKET_STATS\",
          \"s3.region\": \"us-east-1\",
          \"store.url\": \"http://minio:9000\",
          \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",
          \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",
          \"storage.class\": \"io.confluent.connect.s3.storage.S3Storage\",
          \"format.class\": \"io.confluent.connect.s3.format.json.JsonFormat\",
          \"value.converter.schemas.enable\": \"false\",
          \"flush.size\": \"1\",
          \"rotate.interval.ms\": \"60000\",
          \"aws.access.key.id\": \"$MINIO_ACCESS_KEY\",
          \"aws.secret.key\": \"$MINIO_SECRET_KEY\",
          \"aws.credentials.provider.class\": \"com.amazonaws.auth.BasicAWSCredentialsProvider\",
          \"store.kafka.keys\" : \"true\"
        }
      }' http://kafka-connect:8083/connectors;
      echo 'S3 Sink Connector configured successfully!';
      "
    depends_on:
      kafka-connect:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully